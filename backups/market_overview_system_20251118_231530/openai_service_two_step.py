# coding: utf-8
"""
Two-Step OpenAI Service: Analysis ‚Üí Styling

This service separates data analysis from personality styling to maximize Syntra's character.

Architecture:
1. Step 1: Data Analysis (GPT-4o-mini)
   - Execute function calls to get crypto data
   - Structured analysis without personality
   - Fast and cost-effective

2. Step 2: Styling with Persona (GPT-4o)
   - Apply full Syntra personality to analysis
   - Dynamic sarcasm mode
   - Catchphrases and character enforcement
   - Creative and engaging

Benefits:
- ‚úÖ Better personality preservation
- ‚úÖ More consistent character
- ‚úÖ Cleaner separation of concerns
- ‚úÖ Higher quality responses
- ‚ö†Ô∏è Slightly higher cost (but worth it)
"""
import json
import logging
from typing import AsyncGenerator, Optional, List, Dict, Any
from datetime import datetime

from openai import AsyncOpenAI
from sqlalchemy.ext.asyncio import AsyncSession

from config.config import OPENAI_API_KEY, ModelConfig
from config.prompt_selector import get_system_prompt
from config.prompts import get_random_catchphrase
from src.database.crud import add_chat_message, get_chat_history, track_cost
from src.services.openai_service import OpenAIService
from src.services.crypto_tools import CRYPTO_TOOLS, execute_tool


logger = logging.getLogger(__name__)


class TwoStepOpenAIService(OpenAIService):
    """
    Two-step AI service: Analysis ‚Üí Styling

    Optimized for maximum personality preservation
    """

    # Enhanced system prompt for comprehensive data analysis (no personality, but deep analysis)
    ANALYSIS_SYSTEM_PROMPT = """
You are a professional crypto market data analyst. Your job is to provide COMPREHENSIVE and CONTEXT-AWARE analysis.

# Your Tasks:
1. **Gather Data**: Call relevant tools to get all available data
2. **Analyze User Context**: Extract and analyze ALL information user provided (team, events, roadmap, concerns)
3. **Assess Market Context**: Examine market phase, token lifecycle, and trends
4. **Evaluate Risks**: Analyze liquidity, volume, regulatory, and project-specific risks
5. **Identify Patterns**: Look for price action, momentum, and market sentiment
6. **Project Scenarios**: Provide multiple scenarios with criteria for decision-making

# SPECIAL: Market Overview Requests
When user asks about the overall market ("what's happening in crypto", "market overview", "—á—Ç–æ –ø–æ —Ä—ã–Ω–∫—É"):
- CALL: get_market_overview() - returns structured data for BTC, ETH, market metrics, news
- The tool already provides: BTC price/RSI/levels, ETH price, dominance, Fear & Greed, trend, relevant news
- Your output should be MINIMAL - just return the raw JSON data for styling step
- DO NOT write prose, just return: "Market data collected: [JSON summary]"
- The styling step will create the narrative from this data

# Analysis Framework (use even with limited data):

**Technical Layer** (if available):
- Price action and momentum
- Support/resistance levels
- Technical indicators (RSI, MACD, etc.)
- Volume analysis and trends

**Fundamental Layer** (always analyze):
- Market cap and FDV (valuation)
- Liquidity depth (risk of manipulation)
- Trading volume (24h, 6h, 1h trends)
- Token lifecycle phase: Early Launch / Growth / Mature / Declining

**User Context Analysis** (ALWAYS ANALYZE):
- Extract ALL details user mentioned: team, founders, partnerships, upcoming events, product plans
- Analyze project type: Creator-driven (influencer/celebrity), community-driven, VC-backed, anonymous
- Assess roadmap items: Products, utilities, tokenomics changes (buybacks/burns/staking)
- Evaluate hype factors: Media presence, social following, narrative strength
- Identify regulatory/reputation risks: Legal concerns, controversial associations, compliance issues
- Understand user's position: Entry price, current P&L, emotional state (loss/profit/neutral)

**Risk Assessment** (CRITICAL):
- Liquidity risk: <$100k = EXTREME (easy manipulation), $100k-$1M = HIGH, $1M-$10M = MODERATE, >$10M = LOW
- Volume risk: Low volume (<$100k/24h) = pump&dump risk, declining volume = exit liquidity trap
- Project-specific risks: Anonymous team, no product, regulatory concerns, influencer dependency
- News sentiment and fundamental changes
- Market conditions (Fear & Greed Index)

**SCAM DETECTION** (REFUSE to analyze if ALL conditions met):
If a token has ALL of these red flags, REFUSE analysis and warn user:
1. Liquidity < $1,000 USD (extreme manipulation risk)
2. 24h Volume < $500 USD (dead/fake token)
3. No significant market cap or FDV data
RESPONSE: "This token shows extreme scam indicators (liquidity <$1k, volume <$500).
Analysis refused - likely a honeypot, rug pull, or dead project. DO NOT invest."

**Perspectives** (project outcomes):
- Short-term (1-7 days): Based on momentum, volume, news
- Mid-term (1-4 weeks): Based on fundamentals, market cycle
- Long-term (1-3 months): Based on project viability, adoption

# For DEX-only tokens:
- Analyze liquidity stability over time (1h/6h/24h volume trends)
- Assess transaction count (health indicator)
- Check price volatility (5m/1h/6h/24h changes)
- Evaluate chain and DEX (Solana/Raydium more volatile than ETH/Uniswap)
- **Buy/Sell Pressure**: Calculate ratio of buys vs sells (>1.2 = bullish, <0.8 = bearish)
- **Momentum Analysis**: Use 5m/1h/6h/24h price changes to identify trend direction
- **Entry Points**: Calculate specific price levels based on:
  * Current volatility (from price_change data)
  * Volume-weighted support (current price - 2-5% for conservative, 5-10% for aggressive)
  * Buy pressure zones (where buys > sells historically)

# Output Format:
Structure your analysis clearly with sections:
- Data Summary (what we found from tools)
- User Context Summary (what user told us - team, events, plans, position)
- Technical Analysis (if available) OR Price Action Analysis (for DEX)
- Fundamental Assessment (liquidity, volume, market cap, phase, creator influence)
- Risk Analysis (specific risks with data + regulatory + project risks)
- **Fibonacci & Price Levels Analysis** (if fibonacci_levels available in data):
  * Current Fibonacci zone (e.g., "0%-23.6% Near ATL oversold zone")
  * Distance from ATH (percentage)
  * Key support levels (Fibonacci + historical S/R)
  * Key resistance levels (Fibonacci + historical S/R)
  * Interpretation of current position in the cycle
- **Decision Scenarios** (CRITICAL - use scenario_levels from data if available):
  * USE the pre-calculated scenario_levels.scenarios data from tools
  * Scenario A: Bullish Scenario
    - Entry zones: Use scenario_levels.scenarios.bullish_scenario.entry_zone (conservative & aggressive)
    - Stop loss: Use scenario_levels.scenarios.bullish_scenario.stop_loss
    - Targets: Use scenario_levels.scenarios.bullish_scenario.targets (target_1, target_2, target_3)
    - Conditions: Add/expand on scenario_levels.scenarios.bullish_scenario.conditions
  * Scenario B: Bearish Scenario
    - Entry zones: Use scenario_levels.scenarios.bearish_scenario.entry_zone
    - Stop loss: Use scenario_levels.scenarios.bearish_scenario.stop_loss
    - Targets: Use scenario_levels.scenarios.bearish_scenario.targets
    - Conditions: Add/expand on scenario_levels.scenarios.bearish_scenario.conditions
  * Scenario C: Range Trading (if applicable)
    - Range boundaries: Use scenario_levels.scenarios.range_bound_scenario.range
    - Strategy: scenario_levels.scenarios.range_bound_scenario.strategy
  * For EACH scenario: Include risk/reward ratio, probability assessment, decision criteria
  * If scenario_levels NOT available: Generate scenarios manually based on current price ¬±5-10%
- Perspectives (short/mid/long-term based on data and user context)

IMPORTANT RULES:
- NEVER say "buy", "sell", "hold", "average down" as direct advice
- Instead use "Scenario A/B/C" format with criteria for decision-making
- ALWAYS use specific price levels from fibonacci_levels and scenario_levels when available
- If ATH date (ath_date) is in extended_data, ALWAYS mention it with context (e.g., "ATH $2.39 was 11 months ago on Jan 12, 2024")
- Be objective, factual, and thorough. NO personality in this step.
- Focus on actionable insights with clear decision frameworks and CONCRETE price levels
"""

    async def stream_two_step_completion(
        self,
        session: AsyncSession,
        user_id: int,
        user_message: str,
        user_language: str = "ru",
        max_tool_iterations: int = 5,
    ) -> AsyncGenerator[str, None]:
        """
        Two-step streaming completion with enhanced personality

        Step 1: Get data with function calling (mini model, no personality)
        Step 2: Style response with full Syntra persona (4o model, max creativity)

        Args:
            session: Database session
            user_id: User's database ID
            user_message: User's message
            user_language: User's language ('ru' or 'en')
            max_tool_iterations: Max function calling iterations

        Yields:
            Text chunks from final styled response
        """
        try:
            # Save user message to history
            await add_chat_message(
                session, user_id=user_id, role="user", content=user_message
            )

            logger.info(
                f"üé¨ Two-step process started for user {user_id}: {user_message[:50]}..."
            )

            # ==========================================
            # STEP 1: DATA ANALYSIS (mini, no personality)
            # ==========================================

            logger.info("üìä Step 1: Analyzing data with function calling...")

            # Get recent chat history for context (last 5 messages)
            history = await get_chat_history(session, user_id, limit=5)

            # Build analysis messages with context
            analysis_messages: List[Dict[str, Any]] = [
                {"role": "system", "content": self.ANALYSIS_SYSTEM_PROMPT}
            ]

            # Add recent history for context (but not the last message - it's user_message)
            for msg in history[:-1]:  # Exclude last message (it's the current one we just saved)
                analysis_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })

            # Add current user message
            analysis_messages.append({"role": "user", "content": user_message})

            step1_input_tokens = 0
            step1_output_tokens = 0
            tool_calls_made = []
            structured_analysis = ""

            # Iterative function calling loop
            iteration = 0
            while iteration < max_tool_iterations:
                iteration += 1

                # Use mini for data gathering (cost-effective)
                response = await self.client.chat.completions.create(
                    model=ModelConfig.GPT_4O_MINI,
                    messages=analysis_messages,
                    tools=CRYPTO_TOOLS,
                    tool_choice="auto",
                    max_tokens=800,  # Enough for structured analysis
                    temperature=0.3,  # Low temperature for factual analysis
                )

                message = response.choices[0].message

                # Track tokens
                if response.usage:
                    step1_input_tokens += response.usage.prompt_tokens
                    step1_output_tokens += response.usage.completion_tokens

                # Check if AI wants to call tools
                if message.tool_calls:
                    logger.info(f"üîß AI requested {len(message.tool_calls)} tool calls")

                    # Add assistant message with tool calls
                    analysis_messages.append({
                        "role": "assistant",
                        "content": message.content,
                        "tool_calls": [
                            {
                                "id": tc.id,
                                "type": "function",
                                "function": {
                                    "name": tc.function.name,
                                    "arguments": tc.function.arguments,
                                },
                            }
                            for tc in message.tool_calls
                        ],
                    })

                    # Execute tools
                    for tool_call in message.tool_calls:
                        tool_name = tool_call.function.name
                        tool_id = tool_call.id

                        try:
                            arguments = json.loads(tool_call.function.arguments)
                            logger.info(f"‚öôÔ∏è Executing: {tool_name}({arguments})")

                            result = await execute_tool(tool_name, arguments)

                            analysis_messages.append({
                                "role": "tool",
                                "tool_call_id": tool_id,
                                "content": result,
                            })

                            tool_calls_made.append({
                                "name": tool_name,
                                "arguments": arguments,
                            })

                        except Exception as e:
                            logger.error(f"‚ùå Tool execution failed: {e}")
                            analysis_messages.append({
                                "role": "tool",
                                "tool_call_id": tool_id,
                                "content": json.dumps({"success": False, "error": str(e)}),
                            })

                    # Continue to next iteration
                    continue

                else:
                    # No more tools - AI provided structured analysis
                    structured_analysis = message.content or ""
                    logger.info(f"‚úÖ Step 1 complete. Analysis: {len(structured_analysis)} chars")
                    break

            # ==========================================
            # STEP 2: STYLING WITH SYNTRA PERSONA (4o, full creativity)
            # ==========================================

            logger.info("üé® Step 2: Styling with Syntra persona...")

            # Build persona prompt with dynamic sarcasm detection
            syntra_system_prompt = get_system_prompt(
                user_language,
                user_message=user_message  # Auto-detect sarcasm mode
            )

            # Detect if user mentioned losses/drawdown for SAFEGUARD
            # –í–ê–ñ–ù–û: –ù–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ "-" –∫–∞–∫ —Ç—Ä–∏–≥–≥–µ—Ä (–ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è –Ω–∞ "BTC -5%")
            user_has_losses = any(trigger in user_message.lower() for trigger in [
                "–ø—Ä–æ—Å–∞–¥–∫–∞", "—É–±—ã—Ç–æ–∫", "–≤ –º–∏–Ω—É—Å–µ", "–ø–æ—Ç–µ—Ä—è–ª", "—Å–ª–∏–ª", "–ª–∏–∫–≤–∏–¥–Ω—É–ª–∏"
            ])

            safeguard_instruction = ""
            if user_has_losses:
                safeguard_instruction = """
‚ö†Ô∏è SAFEGUARD MODE: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–ø–æ–º—è–Ω—É–ª —É–±—ã—Ç–∫–∏ –∏–ª–∏ –ø—Ä–æ—Å–∞–¥–∫—É.
- –ü–û–õ–ù–û–°–¢–¨–Æ —É–±–µ—Ä–∏ —Å–∞—Ä–∫–∞–∑–º –∏ –∏—Ä–æ–Ω–∏—é
- –ì–æ–≤–æ—Ä–∏ —Å–ø–æ–∫–æ–π–Ω–æ, —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–µ
- –§–æ–∫—É—Å –Ω–∞ –ø–æ–º–æ—â–∏ –ø—Ä–∏–Ω—è—Ç—å –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
"""

            # Detect if this is a market overview question
            is_market_overview = any(keyword in user_message.lower() for keyword in [
                "–ø–æ —Ä—ã–Ω–∫—É", "—Ä—ã–Ω–æ–∫", "—á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç", "–∫–∞–∫ –∫—Ä–∏–ø—Ç–∞", "market",
                "–æ–±–∑–æ—Ä —Ä—ã–Ω–∫–∞", "–æ–±—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è", "–æ–±—â–∏–π —Ç—Ä–µ–Ω–¥"
            ])

            # Create styling prompt based on question type
            if is_market_overview:
                styling_prompt = f"""
–í–æ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ä—ã–Ω–∫–µ –æ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:

{structured_analysis}

{safeguard_instruction}

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –°–û–ó–î–ê–¢–¨ –° –ù–£–õ–Ø –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ Syntra, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞.

üî• –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ò–°–ü–û–õ–¨–ó–£–ô –¢–û –ß–¢–û –†–ï–ê–õ–¨–ù–û –í–ê–ñ–ù–û:

1. **BTC –£–†–û–í–ù–ò** (–∏—Å–ø–æ–ª—å–∑—É–π –µ—Å–ª–∏ –µ—Å—Ç—å):
   - scenario_levels.key_levels.immediate_support/resistance ‚Üí –æ—Å–Ω–æ–≤–Ω—ã–µ —É—Ä–æ–≤–Ω–∏
   - scenario_levels.key_levels.ema_levels ‚Üí —Å distance_pct (–Ω–∞–ø—Ä–∏–º–µ—Ä: "—Ü–µ–Ω–∞ –Ω–∞ 3% –Ω–∏–∂–µ EMA50 ‚Äî –µ—â—ë —Ç–µ—Ä–ø–∏–º–æ")
   - support_resistance.liquidity_zones ‚Üí –∑–æ–Ω—ã —Å –≤—ã—Å–æ–∫–∏–º –æ–±—ä—ë–º–æ–º ("–∑–¥–µ—Å—å —Å–≤–µ—á–∞ +3% –Ω–∞ –æ–±—ä—ë–º–µ x1.8 ‚Äî –∑–æ–Ω–∞ –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç–∏")
   - fibonacci_levels ‚Üí –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ

2. **–í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨** (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞):
   - scenario_levels.atr ‚Üí "ATR $XX ‚Äî –¥–≤–∏–∂–µ–Ω–∏–µ —Ä–≤–∞–Ω–æ–µ/—Å–ø–æ–∫–æ–π–Ω–æ–µ"
   - –î–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏: –∏—Å–ø–æ–ª—å–∑—É–π ATR-based SL/TP –∏–∑ scenarios

3. **–§–¨–Æ–ß–ï–†–°–ù–´–ï –î–ê–ù–ù–´–ï** (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã):
   - funding_rate + funding_sentiment ‚Üí "—Ñ–∞–Ω–¥–∏–Ω–≥ +0.02% (bullish bias)"
   - long_short_ratio + ls_sentiment ‚Üí "–ª–æ–Ω–≥–∏—Å—Ç—ã 1.2:1 (—Ç–æ–ª–ø–∞ –≤ –ª–æ–Ω–≥–∞—Ö)"

4. **MARKET PHASE** (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω):
   - cycle_data.current_band ‚Üí "Rainbow Chart: HODL –∑–æ–Ω–∞" –∏–ª–∏ "Buy zone"

‚ö†Ô∏è –ù–ï –Ω—É–∂–Ω–æ –≤–ø–∏—Ö–∏–≤–∞—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –≤—Å–µ –ø–æ–ª—è –≤ –∫–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç - –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –≤–∞–∂–Ω–æ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

5. **–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê**:
   ```
   BTC –Ω–∞ $XX (+/-Y%), –ø–æ–¥–¥–µ—Ä–∂–∫–∞ $ZZ (EMA50), —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ $WW.
   RSI X, MACD [signal], ATR $1.2k ‚Äî –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å [–Ω–∏–∑–∫–∞—è/—Å—Ä–µ–¥–Ω—è—è/–≤—ã—Å–æ–∫–∞—è].
   [–ï—Å–ª–∏ –µ—Å—Ç—å funding/long-short]: –§–∞–Ω–¥–∏–Ω–≥ +X%, –ª–æ–Ω–≥–∏—Å—Ç—ã X:1 ‚Äî [–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è].
   [–ï—Å–ª–∏ market phase]: Rainbow Chart: [—Ñ–∞–∑–∞] ‚Äî [—á—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç].

   –î–æ–º–∏–Ω–∞—Ü–∏—è BTC X%, ETH X%, –∞–ª—å—Ç—ã X%. F&G: X ([–∂–∞–¥–Ω–æ—Å—Ç—å/—Å—Ç—Ä–∞—Ö/–Ω–µ–π—Ç—Ä–∞–ª]).

   ETH –Ω–∞ $XX (+/-Y%) [–µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ].

   [–¢–æ—Ä–≥–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –µ—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç]:
   Long bias / Short bias / No-trade zone.
   –ï—Å–ª–∏ —Ö–æ—á–µ—à—å [–ª–æ–Ω–≥/—à–æ—Ä—Ç] ‚Äî –∂–¥–∏ [—É—Å–ª–æ–≤–∏–µ] –æ—Ç $XX. SL –∑–∞ $YY (ATR-based). TP —É $ZZ.

   ‚ö° NFA
   ```

üö´ **–ê–ë–°–û–õ–Æ–¢–ù–û –ó–ê–ü–†–ï–©–ï–ù–û**:
- –í—ã–¥—É–º—ã–≤–∞—Ç—å —É—Ä–æ–≤–Ω–∏ –∫–æ—Ç–æ—Ä—ã—Ö –ù–ï–¢ –≤ scenario_levels.key_levels ("–ø–æ–¥–¥–µ—Ä–∂–∫–∞ $77,943" –µ—Å–ª–∏ —ç—Ç–æ–≥–æ —á–∏—Å–ª–∞ –Ω–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö)
- –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞–∂–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (ATR, funding, long/short, market phase) –µ—Å–ª–∏ –æ–Ω–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
- "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç", "—Å–ª–µ–¥–∏—Ç–µ –∑–∞", "–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ", "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é"
- –í–æ–¥–∞ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏

üìä **–§–û–†–ú–ê–¢ –£–†–û–í–ù–ï–ô**:
- –ï—Å–ª–∏ —É—Ä–æ–≤–µ–Ω—å –ï–°–¢–¨ –≤ –¥–∞–Ω–Ω—ã—Ö: "$XX" (–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è —Ü–∏—Ñ—Ä–∞)
- –ï—Å–ª–∏ —É—Ä–æ–≤–Ω—è –ù–ï–¢: "–∑–æ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ ~XX-YYk" (–¥–∏–∞–ø–∞–∑–æ–Ω)
- –ï—Å–ª–∏ —É—Ä–æ–≤–µ–Ω—å = EMA: "$XX (EMA50)" ‚Äî —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫

üéØ **–ï–°–õ–ò –°–ü–†–ê–®–ò–í–ê–Æ–¢ –ü–†–û –¢–û–†–ì–û–í–õ–Æ** ("–ª–æ–Ω–≥–æ–≤–∞—Ç—å/—à–æ—Ä—Ç–∏—Ç—å/—Å—Ç–æ—Ä–æ–Ω–∞"):
- –î–∞–π —á—ë—Ç–∫–∏–π bias: long bias / short bias / no-trade zone
- –ò–°–ü–û–õ–¨–ó–£–ô scenario_levels.scenarios.bullish_scenario –∏–ª–∏ bearish_scenario:
  * entry_zone (conservative/aggressive)
  * stop_loss (conservative/aggressive) - –¥–ª—è –ª–æ–Ω–≥–∞: entry - ATR, –¥–ª—è —à–æ—Ä—Ç–∞: entry + ATR
  * targets (target_1, target_2, target_3)
- –ï—Å–ª–∏ –µ—Å—Ç—å scenario_levels.leverage_recommendation ‚Üí –£–ü–û–ú–Ø–ù–ò –û–°–¢–û–†–û–ñ–ù–û:
  * "–î–ª—è —Ç–∞–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –ø–ª–µ—á–∞ ‚Äî –¥–æ [X]x. –í—Å—ë –≤—ã—à–µ ‚Äî —É–∂–µ –∫–∞–∑–∏–Ω–æ."
  * –ù–∞–ø—Ä–∏–º–µ—Ä: "–ü—Ä–∏ ATR 3.2% (—Å—Ä–µ–¥–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å) ‚Äî –¥–æ 3-5x –º–∞–∫—Å"
  * ‚ö†Ô∏è –≠—Ç–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞—è –∑–æ–Ω–∞ - –¥–∞–≤–∞–π –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã, –Ω–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–≤–µ—Ç—ã
- –û–±—ä—è—Å–Ω–∏ –ü–û–ß–ï–ú–£ (RSI, —Ç—Ä–µ–Ω–¥, funding, long/short, market phase)
- –§–æ—Ä–º–∞—Ç: "–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –ª–æ–Ω–≥ ‚Äî entry –æ—Ç $XX, SL –∑–∞ $YY, TP —É $ZZ."
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û NFA –≤ –∫–æ–Ω—Ü–µ

‚è±Ô∏è **–î–õ–ò–ù–ê**: 180-220 —Å–ª–æ–≤ –º–∞–∫—Å–∏–º—É–º

–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: "{user_message}"
"""
            else:
                styling_prompt = f"""
–í–æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞:

{structured_analysis}

{safeguard_instruction}

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –°–û–ó–î–ê–¢–¨ –° –ù–£–õ–Ø –æ—Ç–≤–µ—Ç –≤ —Å—Ç–∏–ª–µ Syntra, –∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
- –ò—Å–ø–æ–ª—å–∑—É–π –í–°–ï –≤–∞–∂–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–ø–æ–º—è–Ω—É–ª (–∫–æ–º–∞–Ω–¥–∞, —Å–æ–±—ã—Ç–∏—è, –ø–ª–∞–Ω—ã, roadmap)
- –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –í–°–ï —Ä–∏—Å–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –∞–Ω–∞–ª–∏—Ç–∏–∫ –≤—ã—è–≤–∏–ª
- –°–æ—Ö—Ä–∞–Ω–∏ —Ñ–æ—Ä–º–∞—Ç "–°—Ü–µ–Ω–∞—Ä–∏–π –ê/–ë/–í/–ì" - –ù–ò–ö–û–ì–î–ê –Ω–µ –¥–∞–≤–∞–π –ø—Ä—è–º—ã–µ —Å–æ–≤–µ—Ç—ã —Ç–∏–ø–∞ "–¥–µ—Ä–∂–∏", "–ø–æ–∫—É–ø–∞–π", "—É—Å—Ä–µ–¥–Ω—è–π—Å—è"
- –ö–∞–∂–¥—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å: —É—Å–ª–æ–≤–∏—è, —Ä–∏—Å–∫–∏, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è

–§–û–†–ú–ê–¢ (–ë–ï–ó ### –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —Ç–æ–ª—å–∫–æ —ç–º–æ–¥–∑–∏ + —Ç–µ–∫—Å—Ç):
üìä –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
(–¥–∞–Ω–Ω—ã–µ + —É—Ä–æ–≤–Ω–∏)

üì∞ –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
(—á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∞—Å—Å–∫–∞–∑–∞–ª + –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞)

üí° –ú–æ–π –≤–∑–≥–ª—è–¥
(–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ + —Ö–∞—Ä–∞–∫—Ç–µ—Ä, –Ω–æ –ë–ï–ó —Å–∞—Ä–∫–∞–∑–º–∞ –µ—Å–ª–∏ SAFEGUARD)

‚ö†Ô∏è –†–∏—Å–∫–∏
(–≤—Å–µ —Ä–∏—Å–∫–∏: –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å, —Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–µ, –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ)

üéØ –°—Ü–µ–Ω–∞—Ä–∏–∏ —Ä–µ—à–µ–Ω–∏–π
(–ê/–ë/–í/–ì —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏)

‚ö° NFA

–í–ê–ñ–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π ### –∏–ª–∏ –¥—Ä—É–≥–∏–µ markdown –∑–∞–≥–æ–ª–æ–≤–∫–∏. –¢–æ–ª—å–∫–æ —ç–º–æ–¥–∑–∏ + –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ–∫—Ü–∏–∏ + –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫–∏ + –∫–æ–Ω—Ç–µ–Ω—Ç.

–ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å: "{user_message}"
"""

            styling_messages = [
                {"role": "system", "content": syntra_system_prompt},
                {"role": "user", "content": styling_prompt},
            ]

            # Stream styled response with GPT-4o (best model for creativity)
            stream = await self.client.chat.completions.create(
                model=ModelConfig.GPT_4O,  # ‚ö° Always use 4o for styling
                messages=styling_messages,
                max_tokens=ModelConfig.MAX_TOKENS_RESPONSE,
                temperature=ModelConfig.DEFAULT_TEMPERATURE,  # 0.85 for creativity
                stream=True,
            )

            step2_input_tokens = 0
            step2_output_tokens = 0
            full_response = ""

            # Stream response to user
            async for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content

                # Track tokens
                if hasattr(chunk, "usage") and chunk.usage:
                    step2_input_tokens = chunk.usage.prompt_tokens
                    step2_output_tokens = chunk.usage.completion_tokens

            # Estimate tokens if not provided
            if step1_input_tokens == 0:
                step1_input_tokens = self.count_tokens(self.ANALYSIS_SYSTEM_PROMPT + user_message)
            if step1_output_tokens == 0:
                step1_output_tokens = self.count_tokens(structured_analysis)
            if step2_input_tokens == 0:
                step2_input_tokens = self.count_tokens(syntra_system_prompt + styling_prompt)
            if step2_output_tokens == 0:
                step2_output_tokens = self.count_tokens(full_response)

            # Calculate total cost
            step1_cost = self.calculate_cost(
                ModelConfig.GPT_4O_MINI, step1_input_tokens, step1_output_tokens
            )
            step2_cost = self.calculate_cost(
                ModelConfig.GPT_4O, step2_input_tokens, step2_output_tokens
            )
            total_cost = step1_cost + step2_cost

            # Save assistant response to history
            await add_chat_message(
                session, user_id=user_id, role="assistant", content=full_response
            )

            # Track cost
            await track_cost(
                session,
                user_id=user_id,
                service="openai_two_step",
                model=f"mini+4o",
                tokens=step1_input_tokens + step2_input_tokens + step1_output_tokens + step2_output_tokens,
                cost=total_cost,
            )

            logger.info(
                f"‚úÖ Two-step complete for user {user_id}\n"
                f"   Step 1 (mini): {step1_input_tokens}+{step1_output_tokens} tokens, ${step1_cost:.4f}\n"
                f"   Step 2 (4o):   {step2_input_tokens}+{step2_output_tokens} tokens, ${step2_cost:.4f}\n"
                f"   Total: ${total_cost:.4f}, Tools: {len(tool_calls_made)}"
            )

        except Exception as e:
            logger.exception(f"‚ùå Error in two-step completion: {e}")
            yield ""


# Global instance
two_step_service = TwoStepOpenAIService()
